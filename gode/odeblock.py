# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_odeblock.ipynb.

# %% auto 0
__all__ = ['ODEBlock']

# %% ../nbs/04_odeblock.ipynb 3
import dgl
import torch
import torch.nn as nn, torchdiffeq

from .utils import get_torchdiffeq_solver, torch_t

class ODEBlock(nn.Module):
    def __init__(
        self, 
        func:nn.Module, 
        method:str='dopri5', # the ODE solver method 
        rtol:float=1e-3, # relative tolerance for the ODE Solver
        atol:float=1e-4, # absolute tolerances for the ODE Solver 
        adjoint:bool=True # Adjoint sensitivity method 
    ):
        """ Standard ODEBlock class. Can handle all types of ODE functions
            :method:str = {'euler', 'rk4', 'dopri5', 'adams'}
        """
        super().__init__()
        self.func = func
        self.method = method
        self.adjoint = adjoint
        self.atol = atol
        self.rtol = rtol

    def forward(
        self, 
        x:torch.Tensor, # the input nodes feature
        t:torch.Tensor, # the time points over which to solve the ODE 
        return_whole_sequence:bool=False,
        adjoint:bool=None
    ):
        t = torch_t(t)
        t = t.to(x.device).type_as(x) # Moves the time tensor to the same device as the input tensor and ensures it has the same data type 
        
         # Solves the ODE using the chosen solver, applying func (the differential equation) to x over the time span t
        solver = get_torchdiffeq_solver(self.adjoint if adjoint is None else adjoint)     
        out = solver(
            self.func, x, t,
            rtol=self.rtol, atol=self.atol, method=self.method
        ) 
        
        # If return_whole_sequence is False, return only the final time point (last step), otherwise return the entire sequence
        if not return_whole_sequence:
            out = out[-1]
        
        return out
    
    def forward_batched(self, x:torch.Tensor, nn:int, indices:list, timestamps:set):
        """ Modified forward for ODE batches with different integration times """
        t = torch.Tensor(list(timestamps)) # Converts the set of timestamps to a PyTorch tensor
        out = self.forward(x, t, return_whole_sequence=True)  # Solves the ODE and returns the full time sequence      
        out = self._build_batch(out, nn, indices).reshape(x.shape) # Reshapes the ODE output to match the batch's input shape
        return out
    
    def _build_batch(self, odeout, nn, indices): # nn: number of nodes in the batch 
        b_out = []
        for i in range(len(indices)):

            # For each set of indices, append the corresponding portion of the ODE output
            b_out.append(odeout[indices[i], i*nn:(i+1)*nn])

        # Concatenate the batch output and move it to the appropriate device
        return torch.cat(b_out).to(odeout.device)              
        
    def trajectory(self, x:torch.Tensor, t_end:int, num_points:int):
        t = torch.linspace(0, t_end, num_points).type_as(x).to(x.device)
        out = self.forward(x, t, return_whole_sequence=True, adjoint=False)
        return out
